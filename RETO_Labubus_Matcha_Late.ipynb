{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "sourceId": 20561,
          "databundleVersionId": 1187889,
          "sourceType": "competition"
        }
      ],
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "provenance": [],
      "name": "RETO - Labubus Matcha Late",
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Angeltrek/TC3006C-Project/blob/main/RETO_Labubus_Matcha_Late.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "# IMPORTANT: SOME KAGGLE DATA SOURCES ARE PRIVATE\n",
        "# RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES.\n",
        "import kagglehub\n",
        "kagglehub.login()\n"
      ],
      "metadata": {
        "id": "KQHflgoIzrKg"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "covid_segmentation_path = kagglehub.competition_download('covid-segmentation')\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "ivSwq3n2zrKo"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES\n",
        "# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from tempfile import NamedTemporaryFile\n",
        "from urllib.request import urlopen\n",
        "from urllib.parse import unquote, urlparse\n",
        "from urllib.error import HTTPError\n",
        "from zipfile import ZipFile\n",
        "import tarfile\n",
        "import shutil\n",
        "\n",
        "CHUNK_SIZE = 40960\n",
        "DATA_SOURCE_MAPPING = 'covid-segmentation:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-competitions-data%2Fkaggle-v2%2F20561%2F1187889%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240906%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240906T172045Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D43742a5ce41d83ccfc3c8743c5c8079abe9d6551dac4966a23f0bc0a359986061563265bf12ebfe02c15b74ea516fd390ae87498e2b0eeee4d0c4130ff222cb97c8180aec7776cdaa156a56afbb9f3bf08925e57e8ef81a3037b39d68d6aaff017bb0bbe4ccf290b729140201790b7e8858beec45fb63e06ff0e01a0104b2d197c79b64fbc94fc2b8466da932dfe6f4d07651cb02bf789876345aa01ebdc11e32c56c168e4cbc2cd8fd289562cc0e8657c10e99c48d53bf333bb326e02f09e13f8e67438790fc2787521404e3b6ce197637c4398bd5dd04f4f099e48afc7a8b2e837ea63c9aa7ff785a50612b2fadb12bdaca4ed058122abc50b1e866742e2bf'\n",
        "\n",
        "KAGGLE_INPUT_PATH='/kaggle/input'\n",
        "KAGGLE_WORKING_PATH='/kaggle/working'\n",
        "KAGGLE_SYMLINK='kaggle'\n",
        "\n",
        "!umount /kaggle/input/ 2> /dev/null\n",
        "shutil.rmtree('/kaggle/input', ignore_errors=True)\n",
        "os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)\n",
        "os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)\n",
        "\n",
        "try:\n",
        "  os.symlink(KAGGLE_INPUT_PATH, os.path.join(\"..\", 'input'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "try:\n",
        "  os.symlink(KAGGLE_WORKING_PATH, os.path.join(\"..\", 'working'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "\n",
        "for data_source_mapping in DATA_SOURCE_MAPPING.split(','):\n",
        "    directory, download_url_encoded = data_source_mapping.split(':')\n",
        "    download_url = unquote(download_url_encoded)\n",
        "    filename = urlparse(download_url).path\n",
        "    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)\n",
        "    try:\n",
        "        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:\n",
        "            total_length = fileres.headers['content-length']\n",
        "            print(f'Downloading {directory}, {total_length} bytes compressed')\n",
        "            dl = 0\n",
        "            data = fileres.read(CHUNK_SIZE)\n",
        "            while len(data) > 0:\n",
        "                dl += len(data)\n",
        "                tfile.write(data)\n",
        "                done = int(50 * dl / int(total_length))\n",
        "                sys.stdout.write(f\"\\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded\")\n",
        "                sys.stdout.flush()\n",
        "                data = fileres.read(CHUNK_SIZE)\n",
        "            if filename.endswith('.zip'):\n",
        "              with ZipFile(tfile) as zfile:\n",
        "                zfile.extractall(destination_path)\n",
        "            else:\n",
        "              with tarfile.open(tfile.name) as tarfile:\n",
        "                tarfile.extractall(destination_path)\n",
        "            print(f'\\nDownloaded and uncompressed: {directory}')\n",
        "    except HTTPError as e:\n",
        "        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')\n",
        "        continue\n",
        "    except OSError as e:\n",
        "        print(f'Failed to load {download_url} to path {destination_path}')\n",
        "        continue\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "DOaGPSVSWlEK",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-09-17T16:13:05.026076Z",
          "iopub.execute_input": "2025-09-17T16:13:05.026404Z",
          "iopub.status.idle": "2025-09-17T16:13:05.19617Z",
          "shell.execute_reply.started": "2025-09-17T16:13:05.026372Z",
          "shell.execute_reply": "2025-09-17T16:13:05.195469Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Segmentación de imagenes para la detección del COVID-19"
      ],
      "metadata": {
        "id": "T6P7wlUcWlEO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Esta solución esta basada los notebooks siguientes:\n",
        "- [PyTorch Baseline for Semantic Segmentation](https://www.kaggle.com/code/maedemaftouni/pytorch-baseline-for-semantic-segmentation) hecho por Maede Maftouni.\n",
        "- [covid-segmentation](https://www.kaggle.com/code/oskaradolfovillalpez/covid-segmentation) hecho por Oskar Adolfo Villa López."
      ],
      "metadata": {
        "id": "EfpDLzq9SNIk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Team 1:\n",
        "* Angel Mauricio Ramirez Herrera - A01710158\n",
        "* Diego Antonio García Padilla - A01710777\n",
        "* José Eduardo Viveros Escamilla - A01710605\n",
        "* Kevin Alejandro Ramírez Luna - A01711063\n",
        "* Guadalupe Paulina López Cuevas - A01701095\n",
        "* Cristian Chávez Guía - A01710680\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "cMUjBt5yWlEQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **INTRODUCCIÓN**\n",
        "\n",
        "El propósito de este notebook es abordar una problematica crítica que es la segmentación de imágenes CT (tomografías computarizadas)  médicas relacionadas con el COVID-19. El objetivo es identificar y delimitar áreas especificas de los pulmones. Uno de los problemas más comunes en la segmentación es detectar caraterísticas clave como las opacidades en vidrio esmerilado y las consolidaciones, siendo estos indicadores importantes en las infecciones por COVID-19. Generar este tipo de segmentaciones en áreas precisas es fundamental para los diagnósticos médicos ya que esto permite evaluar tanto la gravedad de la infección como información valiosa de en que parte esta la enfermedad.\n",
        "\n",
        "**COVID-19**\n",
        "\n",
        "El COVID-19 ingresa al cuerpo principalmente por vías respiratorias dañando a las células alveolares, reduciendo la capacidad de oxigenación en la sangre.\n",
        "Las imágenes CT (tomografías computarizadas) muestran patrones característicos:\n",
        "\n",
        "- Opacidades en vidrio esmerilado (Ground-glass)\n",
        "    - Hallazgo temprano: Áreas grises y difusas, representan inflamación leve o acumulación de líquido en alvéolos.\n",
        "- Consolidaciones\n",
        "    - Enfermedad más avanzada o grave: Áreas blancas y densas, representan acumulación de líquido y células inflamatorias.\n",
        "\n",
        "Importancia de la segmentación en CT:\n",
        "\n",
        "-\tCuantificar la extensión del daño pulmonar.\n",
        "-\tMonitorear la progresión de la enfermedad.\n",
        "-\tEvaluar la respuesta al tratamiento.\n",
        "-\tAsistir en el diagnóstico rápido.\n",
        "\n",
        "\n",
        "\n",
        "**COMPETENCIA KAGGLE**\n",
        "\n",
        "Objetivo\n",
        "\n",
        "-\tDesarrollar modelos de segmentación médica para detectar zonas infectadas en pulmones a partir de imágenes CT (tomografías computarizadas).\n",
        "\n",
        "Datos disponibles\n",
        "\n",
        "-\tImágenes CT\n",
        "-\tMáscaras anotadas por especialistas que delimitan áreas infectadas.\n",
        "\n",
        "Propósito\n",
        "\n",
        "-\tMejorar la precisión y velocidad del diagnóstico, apoyando  a radiólogos en la lucha contra la pandemia.\n",
        "\n",
        "\n",
        "\n",
        "**IMPLICACIONES ÉTICAS**\n",
        "\n",
        "Las implicaciones éticas en la segmentación de imágenes CT médicas\n",
        "\n",
        "-\tPrivacidad y Confidencialidad:\n",
        "    - Las imágenes contienen información sensible del paciente por lo que es crucial el anonimato de los datos, eliminando metadatos identificables, además de obtener el consentimiento informado para su uso en investigación cumpliendo regularizaciones:\n",
        "        - GDPR (Europa):\n",
        "        - HIPPA (EE.UU):\n",
        "-\tSesgo en datos:\n",
        "    - Es esencial usar datasets diversos y representativos.\n",
        "-\tUso clínico vs experimental\n",
        "    - Los modelos en fase de investigación no deben usarse clínicamente sin validación rigurosa. Aprobación ética y ensayos clínicos para garantizar seguridad y eficacia."
      ],
      "metadata": {
        "id": "x3mGMcnvWlES"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install segmentation_models_pytorch"
      ],
      "metadata": {
        "id": "auyuVE6PvIKu",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-09-17T16:13:05.197699Z",
          "iopub.execute_input": "2025-09-17T16:13:05.197938Z",
          "iopub.status.idle": "2025-09-17T16:14:18.266227Z",
          "shell.execute_reply.started": "2025-09-17T16:13:05.197916Z",
          "shell.execute_reply": "2025-09-17T16:14:18.265536Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n",
        "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n",
        "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "execution": {
          "iopub.status.busy": "2025-09-17T16:14:18.267343Z",
          "iopub.execute_input": "2025-09-17T16:14:18.267637Z",
          "iopub.status.idle": "2025-09-17T16:14:18.534148Z",
          "shell.execute_reply.started": "2025-09-17T16:14:18.267599Z",
          "shell.execute_reply": "2025-09-17T16:14:18.533512Z"
        },
        "id": "PFjJFcKyWlEU",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **ANÁLISIS DE DATOS**\n",
        "\n",
        "**ETL/EDA**\n",
        "\n",
        "El dataset proporcionado por la competencia incluye imágenes CT de pulmones con su respectiva máscara de segmentación. Las imágenes provienen de 2 datasets distintos:\n",
        "\n",
        "**Medseg Dataset:**\n",
        "  \n",
        "- 100 imágenes CT de 40 pacientes\n",
        "- Las respectivas máscaras de segmentación que contienen 4 canales:\n",
        "    - Opacidades de vidrio esmerilado (ground glass)\n",
        "    - Consolidaciones\n",
        "    - Tejidos pulmonares no afectados (Lungs Other)\n",
        "    - Fondo\n",
        "\n",
        "**Radiopaedia Dataset:**\n",
        "\n",
        "- 829 imágenes CT\n",
        "- Misma estructura y mascaras que el dataset de Medseg"
      ],
      "metadata": {
        "id": "AVhUTJKU_CAe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Extracción de datos**\n",
        "\n",
        "Los datos y las máscaras (etiquetas con clase de cada imagen) son extraidos de los repositorios dados, donde las imagener se almacenan en un Numpy Array"
      ],
      "metadata": {
        "id": "a_lxnUNOWlEW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prefix = '/kaggle/input/covid-segmentation/'\n",
        "\n",
        "images_radiopedia = np.load(os.path.join(prefix, 'images_radiopedia.npy')).astype(np.float32)\n",
        "masks_radiopedia = np.load(os.path.join(prefix, 'masks_radiopedia.npy')).astype(np.int8)\n",
        "images_medseg = np.load(os.path.join(prefix, 'images_medseg.npy')).astype(np.float32)\n",
        "masks_medseg = np.load(os.path.join(prefix, 'masks_medseg.npy')).astype(np.int8)\n",
        "\n",
        "test_images_medseg = np.load(os.path.join(prefix, 'test_images_medseg.npy')).astype(np.float32)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-09-17T16:14:18.535561Z",
          "iopub.execute_input": "2025-09-17T16:14:18.535968Z",
          "iopub.status.idle": "2025-09-17T16:14:37.056639Z",
          "shell.execute_reply.started": "2025-09-17T16:14:18.535948Z",
          "shell.execute_reply": "2025-09-17T16:14:37.056089Z"
        },
        "id": "-kJ61QTUWlEX",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "La función **visualize** nos permite ver una matriz donde se despliegan las imagenes originales (en escala de grises) en la fila superior. Cada una de las máscaras se depliega por cada columna correspondiente. En este caso las máscaras que queremos que el modelo prediga son las de ground glass y consolidation que se encuentran en la segunda y tercera fila respectivamente.\n",
        "\n",
        "Cada imagen tiene dimensiones de 520x520 pixeles.\n",
        "Las clases de las máscaras de segmentación son las siguientes:\n",
        "\n",
        "-\tClass 0 - \"ground glass\" - objetivo\n",
        "-\tClass 1 - \"consolidations\" - objetivo\n",
        "-\tClass 2 - \"lungs other\" - no importante\n",
        "-\tClass 3 - \"background\" - no importante"
      ],
      "metadata": {
        "id": "Qq4UoXheWlEY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize(image_batch, mask_batch=None, pred_batch=None, num_samples=8, hot_encode=True):\n",
        "    num_classes = mask_batch.shape[-1] if mask_batch is not None else 0\n",
        "    fix, ax = plt.subplots(num_classes + 1, num_samples, figsize=(num_samples * 2, (num_classes + 1) * 2))\n",
        "\n",
        "    for i in range(num_samples):\n",
        "        ax_image = ax[0, i] if num_classes > 0 else ax[i]\n",
        "        if hot_encode:\n",
        "            ax_image.imshow(image_batch[i,:,:,0], cmap='Greys')\n",
        "        else:\n",
        "            ax_image.imshow(image_batch[i,:,:])\n",
        "        ax_image.set_xticks([])\n",
        "        ax_image.set_yticks([])\n",
        "\n",
        "        if mask_batch is not None:\n",
        "            for j in range(num_classes):\n",
        "                if pred_batch is None:\n",
        "                    mask_to_show = mask_batch[i,:,:,j]\n",
        "                else:\n",
        "                    mask_to_show = np.zeros(shape=(*mask_batch.shape[1:-1], 3))\n",
        "                    mask_to_show[..., 0] = pred_batch[i,:,:,j].cpu().numpy() > 0.5\n",
        "                    mask_to_show[..., 1] = mask_batch[i,:,:,j]\n",
        "                ax[j + 1, i].imshow(mask_to_show, vmin=0, vmax=1)\n",
        "                ax[j + 1, i].set_xticks([])\n",
        "                ax[j + 1, i].set_yticks([])\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-09-17T16:14:37.057328Z",
          "iopub.execute_input": "2025-09-17T16:14:37.057521Z",
          "iopub.status.idle": "2025-09-17T16:14:37.064516Z",
          "shell.execute_reply.started": "2025-09-17T16:14:37.057504Z",
          "shell.execute_reply": "2025-09-17T16:14:37.063945Z"
        },
        "id": "IgLofHlVWlEa",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "visualize(images_radiopedia[30:], masks_radiopedia[30:])"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-09-17T16:14:37.065271Z",
          "iopub.execute_input": "2025-09-17T16:14:37.065878Z",
          "iopub.status.idle": "2025-09-17T16:14:39.114388Z",
          "shell.execute_reply.started": "2025-09-17T16:14:37.065859Z",
          "shell.execute_reply": "2025-09-17T16:14:39.113655Z"
        },
        "id": "N0rp0CeTWlEb",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Como podemos observar en la segunda y tercera fila, las dos clases que deseamos segmentar son áreas muy pequeñas comparadas con la imagen completa, lo que significa que las predicciones seran más complejas de hacer."
      ],
      "metadata": {
        "id": "SFN9wS3JWlEc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Preprocesamiento**\n",
        "\n",
        "En la función **onehot_to_mask** lo que hacemos es que debemos convertir las máscaras de segmentación de “one-hot” a formato “índice de clase”, es decir, cambiamos la máscara de 4 canales a un NumPy array, para mapear con colores cada clase."
      ],
      "metadata": {
        "id": "gOwvHtR_vO_q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def onehot_to_mask(mask, palette):\n",
        "    \"\"\"\n",
        "    Converts a mask (H, W, K) to (H, W, C)\n",
        "    \"\"\"\n",
        "    x = np.argmax(mask, axis=-1)\n",
        "    colour_codes = np.array(palette)\n",
        "    x = np.uint8(colour_codes[x.astype(np.uint8)])\n",
        "    return x\n",
        "\n",
        "palette = [[0], [1], [2],[3]]\n",
        "masks_radiopedia_recover = onehot_to_mask(masks_radiopedia, palette).squeeze()  # shape = (H, W)\n",
        "\n",
        "masks_medseg_recover = onehot_to_mask(masks_medseg, palette).squeeze()  # shape = (H, W)\n",
        "\n",
        "print('Hot encoded mask size: ',masks_radiopedia.shape)\n",
        "print('Paletted mask size:',masks_medseg_recover.shape)\n",
        "\n",
        "visualize(masks_medseg_recover[30:],hot_encode=False)"
      ],
      "metadata": {
        "id": "k5lJaBlyvPiN",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-09-17T16:14:39.115209Z",
          "iopub.execute_input": "2025-09-17T16:14:39.115407Z",
          "iopub.status.idle": "2025-09-17T16:14:44.138933Z",
          "shell.execute_reply.started": "2025-09-17T16:14:39.115391Z",
          "shell.execute_reply": "2025-09-17T16:14:44.138166Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Dataset Train, Validation y Test**\n",
        "\n",
        "Ahora debemos separar los datasets en train y validation, donde para validation tomaremos 24 imagenes del dataset de Medseg, todas las demas imágenes restantes de Medseg y todas las de radiopedia seran para entrenamiento siendo un aprox. de ~800.\n",
        "\n",
        "Para el detaset de test, estas son proporcionadas por la competencia de kaggle."
      ],
      "metadata": {
        "id": "jEnaSc7FvU5V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "masks_radiopedia_recover = onehot_to_mask(masks_radiopedia, palette).squeeze()  # shape = (H, W)\n",
        "masks_medseg_recover = onehot_to_mask(masks_medseg, palette).squeeze()  # shape = (H, W)\n",
        "\n",
        "\n",
        "val_indexes, train_indexes = list(range(24)), list(range(24, 100))\n",
        "\n",
        "train_images = np.concatenate((images_medseg[train_indexes], images_radiopedia))\n",
        "train_masks = np.concatenate((masks_medseg_recover[train_indexes], masks_radiopedia_recover))\n",
        "val_images = images_medseg[val_indexes]\n",
        "val_masks = masks_medseg_recover[val_indexes]\n",
        "\n",
        "batch_size = len(val_masks)\n",
        "\n",
        "del masks_medseg_recover\n",
        "del masks_radiopedia_recover\n",
        "del images_radiopedia\n",
        "del masks_radiopedia\n",
        "del images_medseg\n",
        "del masks_medseg"
      ],
      "metadata": {
        "id": "lTGT1X6gvVu4",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-09-17T16:14:44.139672Z",
          "iopub.execute_input": "2025-09-17T16:14:44.139958Z",
          "iopub.status.idle": "2025-09-17T16:14:49.285611Z",
          "shell.execute_reply.started": "2025-09-17T16:14:44.139932Z",
          "shell.execute_reply": "2025-09-17T16:14:49.284838Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Previamente al entrenamiento del modelo, debido a que el dataset de entrenamiento es menor a ~1000 imágenes, para poder mejorar la variabilidad de imagenes se realizaron las siguientes transformaciones para el dataset de train:\n",
        "\n",
        "-\t**Redimensionamiento**: Cambiar las dimensiones de las imágenes a 256x256 pixeles para tener consistencia en las dimensiones de entrada.\n",
        "-   **Rotaciones**: Con un limite de 360° y con una probabilidad del 90%, para que las imagenes pueda llegar a estar totalmente rotadas.\n",
        "-   **Flip horizontal**: Con una probabilidad el 50%, para voltear de manera horizontal las imágenes.\n",
        "\n",
        "Con estas transformaciones podemos asegurar que el modelo pueda predecir diferentes variaciones, haciéndolo más robusto."
      ],
      "metadata": {
        "id": "deTecJpwWlEc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import albumentations\n",
        "\n",
        "import cv2\n",
        "\n",
        "SOURCE_SIZE = 512\n",
        "TARGET_SIZE = 256\n",
        "\n",
        "\n",
        "train_augs = albumentations.Compose([\n",
        "    albumentations.Rotate(limit=360, p=0.9, border_mode=cv2.BORDER_REPLICATE),\n",
        "    albumentations.RandomSizedCrop((int(SOURCE_SIZE * 0.75), SOURCE_SIZE),\n",
        "                                   (TARGET_SIZE, TARGET_SIZE),\n",
        "                                   interpolation=cv2.INTER_NEAREST),\n",
        "    albumentations.HorizontalFlip(p=0.5),\n",
        "\n",
        "])\n",
        "\n",
        "val_augs = albumentations.Compose([\n",
        "    albumentations.Resize(TARGET_SIZE, TARGET_SIZE, interpolation=cv2.INTER_NEAREST)\n",
        "])"
      ],
      "metadata": {
        "id": "dRJCJqA8vdyT",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-09-17T16:14:49.286504Z",
          "iopub.execute_input": "2025-09-17T16:14:49.286778Z",
          "iopub.status.idle": "2025-09-17T16:14:53.771684Z",
          "shell.execute_reply.started": "2025-09-17T16:14:49.286744Z",
          "shell.execute_reply": "2025-09-17T16:14:53.770793Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Load Dataset**\n",
        "\n",
        "Para la creación del objeto **Dataset**, creamos tres métodos además del constructor:\n",
        "\n",
        "- **__getitem__**: Nos devuelve un elemento del dataset despues de transformar la imagen a un tensor y también aplica la tranformación y normalización de las imagenes.\n",
        "- **__len__**: Devuelve la longitud del dataset\n",
        "- **tiles**: Devuelve dos tensors que contienen una imagen y su máscara, donde cada elemento corresponde a un parche de la imagen orginal."
      ],
      "metadata": {
        "id": "Mi-ACbMOvhzn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import segmentation_models_pytorch as smp\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "from tqdm.notebook import tqdm\n",
        "import time\n",
        "\n",
        "from torchvision import transforms as T\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch\n",
        "\n",
        "class Dataset:\n",
        "    def __init__(\n",
        "            self,\n",
        "            images,\n",
        "            masks,\n",
        "            augmentations=None\n",
        "    ):\n",
        "        self.images = images\n",
        "        self.masks = masks\n",
        "        self.augmentations = augmentations\n",
        "        self.mean = [0.485]\n",
        "        self.std = [0.229]\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        image = self.images[i]\n",
        "        mask = self.masks[i]\n",
        "\n",
        "\n",
        "        if self.augmentations is not None:\n",
        "            sample = self.augmentations(image=image, mask=mask)\n",
        "\n",
        "            image, mask = Image.fromarray(np.squeeze(sample['image'], axis=2)), sample['mask']\n",
        "\n",
        "        if self.augmentations is None:\n",
        "            image = Image.fromarray(image)\n",
        "\n",
        "        t = T.Compose([T.ToTensor(), T.Normalize(self.mean, self.std)])\n",
        "        image = t(image)\n",
        "        mask = torch.from_numpy(mask).long()\n",
        "\n",
        "        return image, mask\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def tiles(self, image, mask):\n",
        "\n",
        "        img_patches = image.unfold(1, 512, 512).unfold(2, 768, 768)\n",
        "        img_patches  = img_patches.contiguous().view(3,-1, 512, 768)\n",
        "        img_patches = img_patches.permute(1,0,2,3)\n",
        "\n",
        "        mask_patches = mask.unfold(0, 512, 512).unfold(1, 768, 768)\n",
        "        mask_patches = mask_patches.contiguous().view(-1, 512, 768)\n",
        "\n",
        "        return img_patches, mask_patches\n",
        "\n",
        "\n",
        "train_dataset = Dataset(train_images, train_masks, train_augs)\n",
        "val_dataset = Dataset(val_images, val_masks, val_augs)\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)"
      ],
      "metadata": {
        "id": "38ry36BovjDG",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-09-17T16:14:53.774434Z",
          "iopub.execute_input": "2025-09-17T16:14:53.774743Z",
          "iopub.status.idle": "2025-09-17T16:14:59.688485Z",
          "shell.execute_reply.started": "2025-09-17T16:14:53.774724Z",
          "shell.execute_reply": "2025-09-17T16:14:59.687931Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Funciones para visualización y análisis**\n",
        "\n",
        "Antes de definir el modelo, necesitamos funciones que ayuden a visualizar y analizar el rendimiento del modelo."
      ],
      "metadata": {
        "id": "62-vijj9v7ge"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "En la función **mask_to_onehot** convertimos la máscara de segmentación a One_hot encoded. Esto es una representación binaria donde cada clase se representa con un vector que tiene valor de 1 en la posición correspondiente a la clase y 0 en todas las demás.\n",
        "\n",
        "- Crea una lista vacia **sematic_map** para almacenar los mapas de clases de cada color.\n",
        "- Para cada uno, la máscara se compara con dicho color, generando una matriz booleana que indica que pixel coincide con ese color. Se verifica así todos los canales de color coinciden, generando un mapa binario, True el pixel pertenece a la clase que corresponde al color.\n",
        "- Los mapas de clase para cada color se apilan en una nueva dimesión, donde **semantic_map** se convierte en una matriz de dimensiones (H, W, K), K es el num de clases.\n",
        "- La función regresa un tensor donde cada pixel esta representado por un vector one-hot que indica su clase.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "JuiHVdSTwISY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def mask_to_onehot(mask, palette):\n",
        "    \"\"\"\n",
        "    Converts a segmentation mask (H, W, C) to (H, W, K) where the last dim is a one\n",
        "    hot encoding vector, C is usually 1 or 3, and K is the number of class.\n",
        "    \"\"\"\n",
        "    semantic_map = []\n",
        "    for colour in palette:\n",
        "        #print('colour',colour)\n",
        "        equality = np.equal(mask, colour)\n",
        "        #print('equality',equality)\n",
        "        class_map = np.all(equality, axis=-1)\n",
        "        semantic_map.append(class_map)\n",
        "    semantic_map = np.stack(semantic_map, axis=-1).astype(np.float32)\n",
        "    return torch.from_numpy(semantic_map)\n",
        "\n",
        "i,train_data = next(enumerate(train_dataloader))\n",
        "\n",
        "mask_hot_encoded = mask_to_onehot(torch.unsqueeze(train_data[1],-1).numpy(),palette)\n",
        "#visualize(torch.unsqueeze(torch.squeeze(train_data[0],1),-1),mask_hot_encoded)\n",
        "visualize(train_data[0].permute(0, 2, 3,1),mask_hot_encoded)\n"
      ],
      "metadata": {
        "id": "TUgWHJz9wFQv",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-09-17T16:14:59.689144Z",
          "iopub.execute_input": "2025-09-17T16:14:59.689342Z",
          "iopub.status.idle": "2025-09-17T16:15:01.27836Z",
          "shell.execute_reply.started": "2025-09-17T16:14:59.689325Z",
          "shell.execute_reply": "2025-09-17T16:15:01.277665Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PIXEL ACCURACY**\n",
        "\n",
        "En la función **pixel_accuracy** lo que calculamos es la exactitud por pixel para la máscara de segmentación predicha comparada con la máscara de referencia (ground truth). Esta métrica mide el porcentaje de pixeles en la imagen que coinciden adecuadamente con su clasificación, es decir, si la etiqueta predicha es igual a la etiqueta de referencia.\n",
        "\n",
        "- La salida del modelo la pasamos por **softmax** para convertirla en probabilidades de clases en cad posición de pixel.\n",
        "- Seleccionamos la clase con mayor probabilidad usando **torch.argmax*\n",
        "- Se compara la máscara predicha con la máscara de referencia y se genera una matriz binaria:\n",
        "    - 1 prediccion correcta\n",
        "    - 0 prediccion incorrecta.\n",
        "- Se suman todas las predicciones correctas y se dividen entre el núm total de pixeles en la máscara para calcular la exactitud por pixel.\n",
        "- La función devuelve un valor float.\n",
        "\n"
      ],
      "metadata": {
        "id": "KmYrukY2wKMt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def pixel_accuracy(output, mask):\n",
        "    with torch.no_grad():\n",
        "        output = torch.argmax(F.softmax(output, dim=1), dim=1)\n",
        "        correct = torch.eq(output, mask).int()\n",
        "        accuracy = float(correct.sum()) / float(correct.numel())\n",
        "    return accuracy"
      ],
      "metadata": {
        "id": "ZVFWdyarwNUB",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-09-17T16:15:01.27919Z",
          "iopub.execute_input": "2025-09-17T16:15:01.279437Z",
          "iopub.status.idle": "2025-09-17T16:15:01.284439Z",
          "shell.execute_reply.started": "2025-09-17T16:15:01.279418Z",
          "shell.execute_reply": "2025-09-17T16:15:01.283707Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "**INTERSECCIÓN SOBRE LA UNIÓN (ioU)**\n",
        "\n",
        "Esta función calcula la **Intersección sobre la Unión (ioU)** entre la mascara predicha y la mascara de referencia, con el fin de medir la superposición entre las etiquetas predichas y las etiquetas reales de cada clase.\n",
        "- La máscara predicha y la máscara de referencia se aplanan, se mueven al CPU y se convierten en Numpy arrays.\n",
        "- Convierte la máscara predicha en un formato de etiqueta usando **ToLabel()**.\n",
        "- Se crea una matriz nueva (agg) sumando la máscara predicha y la de referencia. Cada pixel representa una combinación de la clase predicha y la real.\n",
        "- Se calcula el núm de pixeles donde predicción y real son 1, representando la intersección (i).\n",
        "- La unión (u) se calcula como el num de pixeles donde predicción y real son mayor que 0, representando donde la clase fue predicha o esta presente en la referencia.\n",
        "    - Si u==0, la IoU se establece como 1 (coincidencia perfecta)\n",
        "    - De lo contrario, IoU se calcula como i/u\n"
      ],
      "metadata": {
        "id": "WYi5BZxUwSL7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def iou(pred, gt):\n",
        "    pred = pred.squeeze().cpu().data.numpy()\n",
        "    pred = ToLabel(pred)\n",
        "    gt = gt.squeeze().cpu().data.numpy()\n",
        "    agg = pred + gt\n",
        "    i = float(np.sum(agg == 2))\n",
        "    u = float(np.sum(agg > 0))\n",
        "    if u == 0:\n",
        "        result = 1\n",
        "    else:\n",
        "        result = i/u\n",
        "    return result"
      ],
      "metadata": {
        "id": "6bF9GHOswUM1",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-09-17T16:15:01.285297Z",
          "iopub.execute_input": "2025-09-17T16:15:01.28547Z",
          "iopub.status.idle": "2025-09-17T16:15:01.310842Z",
          "shell.execute_reply.started": "2025-09-17T16:15:01.285455Z",
          "shell.execute_reply": "2025-09-17T16:15:01.310265Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "**MEDIA DE LA INTERSECCIÓN SOBRE LA UNIÓN (mIoU)**\n",
        "\n",
        "El **Mean Intersection over Union (mIoU)** es una métrica que calcula el promedio de la **Intersección sobre Union (ioU)** entre etiquetas predichas y etiquetas reales.\n",
        "\n",
        "- La salida del modelo la pasamos por softmax para convertirla en probabilidades de clases en cad posición de pixel.\n",
        "- Seleccionamos la clase con mayor probabilidad usando torch.argmax para crear la máscara predicha.\n",
        "- La máscara predicha y la de referencia se transforman el vetores unidimensionales.\n",
        "- Se inicializa una lista para almacenar el ioU de cada clase.\n",
        "- La función identifica los pixeles de la máscara predicha que pertenecen a la clase actual y los pixeles en la máscara de referencia.\n",
        "    - Si no hay pixeles de la clase actual en la máscara de referencia, se agrega un np.nan a la lista de ioU para esa clase.\n",
        "    - Caso contrario, se calcula la intersección y la unión.\n",
        "- La ioU para esa clase se saca dividiendo la intersección entre la unión.\n",
        "- La función devuelve el promedio de ioU par todas las clases, sin contar los nan.\n"
      ],
      "metadata": {
        "id": "usfeuLXwwQKC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def mIoU(pred_mask, mask, smooth=1e-10, n_classes=4):\n",
        "    with torch.no_grad():\n",
        "        pred_mask = F.softmax(pred_mask, dim=1)\n",
        "        pred_mask = torch.argmax(pred_mask, dim=1)\n",
        "        pred_mask = pred_mask.contiguous().view(-1)\n",
        "        mask = mask.contiguous().view(-1)\n",
        "\n",
        "        iou_per_class = []\n",
        "        for clas in range(0, n_classes): #loop per pixel class\n",
        "            true_class = pred_mask == clas\n",
        "            true_label = mask == clas\n",
        "\n",
        "            if true_label.long().sum().item() == 0: #no exist label in this loop\n",
        "                iou_per_class.append(np.nan)\n",
        "            else:\n",
        "                intersect = torch.logical_and(true_class, true_label).sum().float().item()\n",
        "                union = torch.logical_or(true_class, true_label).sum().float().item()\n",
        "\n",
        "                iou = (intersect + smooth) / (union +smooth)\n",
        "                iou_per_class.append(iou)\n",
        "        return np.nanmean(iou_per_class)"
      ],
      "metadata": {
        "id": "rq3j5vNGwOcA",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-09-17T16:15:01.311621Z",
          "iopub.execute_input": "2025-09-17T16:15:01.311923Z",
          "iopub.status.idle": "2025-09-17T16:15:01.325581Z",
          "shell.execute_reply.started": "2025-09-17T16:15:01.311877Z",
          "shell.execute_reply": "2025-09-17T16:15:01.325056Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Arquitectura del Modelo**\n",
        "\n",
        "En este caso se opto por usar variantes de la arquitectura U-Net:\n",
        "\n",
        "Efficientnet-B2 esta diseñada par lograr un equilibrio óptimo entre eficiencia computacional y precisión, utiliza una técnica llamada “compound scaling” para ajustar la profundidad (núm de capas), el ancho y la resolución del modelo.\n",
        "\n",
        "U-net++ es una evolución de u-Net que mejora la segmentación de estructuras médicas complejas. Debido a sus conexiones anidadas y densas esto permite una mejor integración de características de diferentes escalas. Es ideal para imágenes CT donde los detalles finos son críticos.\n",
        "\n",
        "-\tEncoder (Downsampling):\n",
        "    - Extrae características mediante bloques convolucionales y reduce la resolución espacial.\n",
        "    - 4 niveles de downsampling\n",
        "-\tDecoder (Upsampling):\n",
        "    - Recupera resolución original combinando características den encoder y las conexiones densas.\n",
        "    - 4 niveles de upsampling\n",
        "-\tConexiones anidadas:\n",
        "    - Conexiones densas entre todos los bloques del mismo nivel.\n",
        "\n",
        "**Detalles de EfficientNet-B2 (ENCODER)**:\n",
        "\n",
        "Funciona como “extractor de caracteristicas”, utiliza una técnica llamada “escalado compuesto” que ajusta simultaneamente:\n",
        "- Profundidad (núm de capas)\n",
        "- Ancho (núm de filtros)\n",
        "- Resolución (tamaño de entrada)\n",
        "\n",
        "Mira detalles finos (bordes, texturas pequeñas) y a medida que baja el nivel detecta formas grandes (órganos completos). En vez de tijeras grandes (Max pooling), convoluciones inteligentes para reducir el tamaño.\n",
        "\n",
        "-\tCaracterísticas:\n",
        "    - Número de capas: 16 capas convolucionales\n",
        "    - Kernel size: 3x3\n",
        "    - Stride: capas iniciales 1 (reducción rápida de dimensionalidad), capas posteriores 2 (mantener resolución durante extracción profunda)\n",
        "    - Padding: usa “same” en la mayoría de las capas, tamaño de salida permanece igual que el de la entrada.\n",
        "    - Max Pooling: No usa max pooling, se basa en stride para conservar resolución, mejora eficiencia computacional.\n",
        "    - Activación: ReLU\n",
        "\n",
        "**Detalles de U-Net++ (DECODER)**:\n",
        "\n",
        "-\tCaracterísticas:\n",
        "    - Número de capas: 5 por rama de decoder\n",
        "    - Kernel size: 3x3 en todas las capas\n",
        "    - Stride: 1\n",
        "    - Padding: Usa padding “same”, el tamaño de salida permanece igual al de la entrada, conserva resolución.\n",
        "    - Upsampling: Se usa transposed convolution, kernel 2x2 y stride de 2\n",
        "    - Skip Connections: Pasan por bloques convolucionales adicionales para alienar características antes de fusionarlas.\n",
        "\n",
        "**Capas finales:**\n",
        "\n",
        "-\tActivación:\n",
        "    - Softmax, multiclase exclusivo\n",
        "\n",
        "**Función de perdida**\n",
        "\n",
        "-\tCross-Entropy Loss:\n",
        "    - Penaliza predicciones incorrectas por pixel\n",
        "\n",
        "\n",
        "Los puentes tienen capas intermedias de convolucion adicionales: limpia y refina la informacion antes de fusionarla."
      ],
      "metadata": {
        "id": "ApXDBI_2wZIZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = smp.UnetPlusPlus('efficientnet-b2',in_channels=1, encoder_weights='imagenet',classes=4, activation=None, encoder_depth=5, decoder_channels=[256, 128, 64, 32, 16], decoder_use_batchnorm=True)\n",
        "#decoder_attention_type ='scse'"
      ],
      "metadata": {
        "id": "umCs8Ztnwat8",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-09-17T16:15:01.326413Z",
          "iopub.execute_input": "2025-09-17T16:15:01.32663Z",
          "iopub.status.idle": "2025-09-17T16:15:03.031632Z",
          "shell.execute_reply.started": "2025-09-17T16:15:01.326606Z",
          "shell.execute_reply": "2025-09-17T16:15:03.03112Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **FUNCIONES DE ENTRENAMIENTO**\n",
        "\n",
        "**Función de entrenamiento**: Definimos la función de entrenamiento, donde además de entrenar también evalúa el modelo con un conjunto de datos (train) durante un número específico de epochs.\n",
        "\n",
        "**Función de pérdida**: Criterion representa la función de perdida que se utilizara para entrenar y ajustar los pesos del modelo con Cross Entropy.\n",
        "\n",
        "**Optimizador, Learning rate y Scheduler:**\n",
        "\n",
        "-\tOptimizer, recibe optimizer como parámetro, puede ser Adam o SGD, ajusta los pesos del modelo basándose en los gradientes, en este caso usaremos Adam.\n",
        "-\tLearning rate y Scheduler, se controla por el optimizador y se va a ajustar dinámicamente con un scheduler. La función get_lr() obtiene el learning rate actual y lo almacena en una lista.\n",
        "\n",
        "**Numero de epochs y tamaño de minibatch:**\n",
        "\n",
        "-\tNumero de epochs define cuantas veces el modelo entrenara sobre un conjunto de datos completos.\n",
        "-\tMinibatch, El tamaño de minibatch se determinaran usando train_loader y  val_loader, se define el tamaño al crear dataLoader.\n",
        "\n",
        "**Técnica de regularización:**\n",
        "\n",
        "-\tEarly stopping: Si la pérdida de val no mejora después de 7 epochs seguidas, se detiene el entrenamiento, evitando overfitting (sobreajuste)\n",
        "-\tGuardado del mejor modelo: El modelo se guarda cada que la perdidad de val disminuya, con esto garantizamos guardar el mejor modelo.\n",
        "\n",
        "\n",
        "# **PROCESO DE ENTRENAMIENTO**\n",
        "\n",
        "**Entrenamiento:**\n",
        "\n",
        "-\tEl modelo se pone en entrenamiento con model.train().\n",
        "-\tCada minibatch de imágenes y máscaras pasan por el modelo\n",
        "-\tSe calcula la perdida y se hace back propagation con los.backward()\n",
        "-\tEl optimizador actualiza los pesos con optimizar.step()\n",
        "-\tEl Scheduler ajusta el learning rate con scheduler.step()\n",
        "\n",
        "**Validación:**\n",
        "\n",
        "-\tDespués de cada epoch, el modelo se evalua en el dataset de validation con model.eval() sin actualizar pesos.\n",
        "-\tSe calculan las méttricas mIoU y pixel accuracy para ambos datasets.\n"
      ],
      "metadata": {
        "id": "Qx8ou3HjzrK_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_lr(optimizer):\n",
        "    for param_group in optimizer.param_groups:\n",
        "        return param_group['lr']\n",
        "\n",
        "def fit(epochs, model, train_loader, val_loader, criterion, optimizer, scheduler, patch=False):\n",
        "    #torch.cuda.empty_cache()\n",
        "    train_losses = []\n",
        "    test_losses = []\n",
        "    val_iou = []; val_acc = []\n",
        "    train_iou = []; train_acc = []\n",
        "    lrs = []\n",
        "    min_loss = np.inf\n",
        "    decrease = 1 ; not_improve=0\n",
        "\n",
        "    model.to(device)\n",
        "    fit_time = time.time()\n",
        "    for e in range(epochs):\n",
        "        since = time.time()\n",
        "        running_loss = 0\n",
        "        iou_score = 0\n",
        "        accuracy = 0\n",
        "        #training loop\n",
        "        model.train()\n",
        "        for i, data in enumerate(tqdm(train_loader)):\n",
        "            #training phase\n",
        "            image_tiles, mask_tiles = data\n",
        "\n",
        "            image = image_tiles.to(device); mask =mask_tiles.to(device);\n",
        "            #forward\n",
        "            output = model(image)\n",
        "\n",
        "            loss = criterion(output, mask)\n",
        "            #evaluation metrics\n",
        "            iou_score += mIoU(output, mask)\n",
        "            accuracy += pixel_accuracy(output, mask)\n",
        "            #backward\n",
        "            loss.backward()\n",
        "            optimizer.step() #update weight\n",
        "            optimizer.zero_grad() #reset gradient\n",
        "\n",
        "            #step the learning rate\n",
        "            lrs.append(get_lr(optimizer))\n",
        "            scheduler.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "\n",
        "        else:\n",
        "            model.eval()\n",
        "            test_loss = 0\n",
        "            test_accuracy = 0\n",
        "            val_iou_score = 0\n",
        "            #validation loop\n",
        "            with torch.no_grad():\n",
        "                for i, data in enumerate(tqdm(val_loader)):\n",
        "                    image_tiles, mask_tiles = data\n",
        "\n",
        "                    image = image_tiles.to(device); mask =mask_tiles.to(device);\n",
        "                    output = model(image)\n",
        "                    #evaluation metrics\n",
        "                    val_iou_score +=  mIoU(output, mask)\n",
        "                    test_accuracy += pixel_accuracy(output, mask)\n",
        "                    #loss\n",
        "                    loss = criterion(output, mask)\n",
        "                    test_loss += loss.item()\n",
        "\n",
        "            #calculatio mean for each batch\n",
        "            train_losses.append(running_loss/len(train_loader))\n",
        "            test_losses.append(test_loss/len(val_loader))\n",
        "\n",
        "\n",
        "            if min_loss > (test_loss/len(val_loader)):\n",
        "                print('Loss Decreasing.. {:.3f} >> {:.3f} '.format(min_loss, (test_loss/len(val_loader))))\n",
        "                min_loss = (test_loss/len(val_loader))\n",
        "                decrease += 1\n",
        "                if decrease % 5 == 0:\n",
        "                    print('saving model...')\n",
        "                    torch.save(model, 'Unet_efficientnet_b2_mIoU-{:.3f}.pt'.format(val_iou_score/len(val_loader)))\n",
        "\n",
        "\n",
        "            if (test_loss/len(val_loader)) > min_loss:\n",
        "                not_improve += 1\n",
        "                min_loss = (test_loss/len(val_loader))\n",
        "                print(f'Loss Not Decrease for {not_improve} time')\n",
        "                if not_improve == 7:\n",
        "                    print('Loss not decrease for 7 times, Stop Training')\n",
        "                    break\n",
        "\n",
        "            #iou\n",
        "            val_iou.append(val_iou_score/len(val_loader))\n",
        "            train_iou.append(iou_score/len(train_loader))\n",
        "            train_acc.append(accuracy/len(train_loader))\n",
        "            val_acc.append(test_accuracy/ len(val_loader))\n",
        "            print(\"Epoch:{}/{}..\".format(e+1, epochs),\n",
        "                  \"Train Loss: {:.3f}..\".format(running_loss/len(train_loader)),\n",
        "                  \"Val Loss: {:.3f}..\".format(test_loss/len(val_loader)),\n",
        "                  \"Train mIoU:{:.3f}..\".format(iou_score/len(train_loader)),\n",
        "                  \"Val mIoU: {:.3f}..\".format(val_iou_score/len(val_loader)),\n",
        "                  \"Train Acc:{:.3f}..\".format(accuracy/len(train_loader)),\n",
        "                  \"Val Acc:{:.3f}..\".format(test_accuracy/len(val_loader)),\n",
        "                  \"Time: {:.2f}m\".format((time.time()-since)/60))\n",
        "\n",
        "    history = {'train_loss' : train_losses, 'val_loss': test_losses,\n",
        "               'train_miou' :train_iou, 'val_miou':val_iou,\n",
        "               'train_acc' :train_acc, 'val_acc':val_acc,\n",
        "               'lrs': lrs}\n",
        "    print('Total time: {:.2f} m' .format((time.time()- fit_time)/60))\n",
        "    return history"
      ],
      "metadata": {
        "id": "4bo1eXJ7wgcP",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-09-17T16:15:03.032442Z",
          "iopub.execute_input": "2025-09-17T16:15:03.032688Z",
          "iopub.status.idle": "2025-09-17T16:15:03.044187Z",
          "shell.execute_reply.started": "2025-09-17T16:15:03.03267Z",
          "shell.execute_reply": "2025-09-17T16:15:03.043525Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "**CONSIDERACIONES DE HARDWARE**\n",
        "\n",
        "Para el entrenamiento se uso la GPU para hacer el proceso mucho mas rápido, paralelizando los procesos computacionales. En este caso para los entrenamientos hechos en Kaggle se uso el acelerador de GPU P100."
      ],
      "metadata": {
        "id": "pfgbUwgO5kEM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('Using device:', device)\n",
        "print()"
      ],
      "metadata": {
        "id": "SzmWC_at5jHY",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-09-17T16:15:03.044727Z",
          "iopub.execute_input": "2025-09-17T16:15:03.044965Z",
          "iopub.status.idle": "2025-09-17T16:15:03.107819Z",
          "shell.execute_reply.started": "2025-09-17T16:15:03.04495Z",
          "shell.execute_reply": "2025-09-17T16:15:03.107228Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Métricas de Evaluación**\n",
        "\n",
        "**ENCONTRAR LEARNING RATE**\n",
        "\n",
        "Para encontrar el mejor learning rate el modelo usa diferentes valores desde $ 1 \\times 10^{-8} $ to 10. Esta técnica es inspirada en un artículo publicado por Leslie N. Smith, titulado *Cyclical Learning Rates for Training Neural Networks* (Tasas de Aprendizaje Cíclicas para Entrenar Redes Neuronales). Los valores de la perdida se almacenan en cada iteración usando una función para reducir el ruido."
      ],
      "metadata": {
        "id": "Bs82SIk2Ddl6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def find_lr(model, train_loader, criterion, optimizer, init_value=1e-8, final_value=10.0, beta=0.98):\n",
        "    num_iter = len(train_loader)\n",
        "    lr_values = []\n",
        "    losses = []\n",
        "    avg_loss = 0\n",
        "    best_loss = float('inf')\n",
        "\n",
        "    model.to(device)\n",
        "\n",
        "    for i, (images, masks) in enumerate(tqdm(train_loader)):\n",
        "        images = images.to(device)\n",
        "        masks = masks.to(device)\n",
        "\n",
        "        lr = init_value * (final_value / init_value) ** (i / num_iter)\n",
        "        optimizer.param_groups[0]['lr'] = lr\n",
        "        lr_values.append(lr)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, masks)\n",
        "        avg_loss = beta * avg_loss + (1 - beta) * loss.item()\n",
        "        smoothed_loss = avg_loss / (1 - beta ** (i + 1))\n",
        "        losses.append(smoothed_loss)\n",
        "\n",
        "        if smoothed_loss > 4 * best_loss or torch.isnan(loss):\n",
        "            return lr_values, losses\n",
        "\n",
        "        if smoothed_loss < best_loss:\n",
        "            best_loss = smoothed_loss\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    return lr_values, losses\n",
        "\n",
        "def plot_lr_finder(lr_values, losses):\n",
        "    \"\"\"\n",
        "    Plots the learning rate finder results.\n",
        "\n",
        "    Args:\n",
        "        lr_values: A list of learning rates.\n",
        "        losses: A list of losses.\n",
        "    \"\"\"\n",
        "    plt.plot(lr_values, losses)\n",
        "    plt.xscale('log')\n",
        "    plt.xlabel('Learning Rate')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Learning Rate Finder')\n",
        "    plt.show()\n",
        "\n",
        "model_lr = smp.UnetPlusPlus('efficientnet-b2',in_channels=1, encoder_weights='imagenet',classes=4, activation=None, encoder_depth=5, decoder_channels=[256, 128, 64, 32, 16], decoder_use_batchnorm=True)\n",
        "#decoder_attention_type ='scse'\n",
        "weight_decay = 1e-4\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.AdamW(model_lr.parameters(), weight_decay=weight_decay)\n",
        "# Find the optimal learning rate\n",
        "lr_values, losses = find_lr(model_lr, train_dataloader, criterion, optimizer)\n",
        "\n",
        "# Plot the results\n",
        "plot_lr_finder(lr_values, losses)\n"
      ],
      "metadata": {
        "id": "29N59v5b4Toy",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-09-17T16:15:03.108704Z",
          "iopub.execute_input": "2025-09-17T16:15:03.10902Z",
          "iopub.status.idle": "2025-09-17T16:15:17.383583Z",
          "shell.execute_reply.started": "2025-09-17T16:15:03.108996Z",
          "shell.execute_reply": "2025-09-17T16:15:17.38292Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "**CAMBIO DE LA PERDIDA & LEARNING RATE**\n",
        "\n",
        "Para visualizar de mejor manera los datos podemos usar la función de cambio de la perdida. El objetivo es encontrar el punto donde la perdidad decrece lo más posible, que es donde sacaremos el mayor learning rate."
      ],
      "metadata": {
        "id": "69CCoR1OF6vo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the derivative of the loss\n",
        "diff = np.diff(losses)\n",
        "\n",
        "# Adjust lr_values to match the length of diff\n",
        "adjusted_lr_values = lr_values[1:]\n",
        "\n",
        "# Plot the derivative\n",
        "plt.plot(adjusted_lr_values, diff)\n",
        "plt.xscale('log')\n",
        "plt.xlabel('Learning Rate')\n",
        "plt.ylabel('Change of Loss')\n",
        "plt.title('Rate of change of Loss')\n",
        "plt.show()\n",
        "\n",
        "# Find the index of the minimum derivative\n",
        "min_diff_index = np.argmin(diff)\n",
        "\n",
        "# Find the corresponding learning rate\n",
        "min_diff_lr = adjusted_lr_values[min_diff_index]\n",
        "\n",
        "# Print the learning rate corresponding to the lowest derivative of the loss\n",
        "print(\"Learning Rate corresponding to the highest negative change of the loss:\", min_diff_lr)\n"
      ],
      "metadata": {
        "id": "J2Rg_QXF4913",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-09-17T16:15:17.384352Z",
          "iopub.execute_input": "2025-09-17T16:15:17.384558Z",
          "iopub.status.idle": "2025-09-17T16:15:17.579314Z",
          "shell.execute_reply.started": "2025-09-17T16:15:17.384541Z",
          "shell.execute_reply": "2025-09-17T16:15:17.578627Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **ENTRENAMIENTO**\n",
        "\n",
        "El mejor learning rate ya fue encontrado con la funcion anterior. El número de epochs fue encontrado a travez de prueba y error, 15 fue el valor donde el overfitting (sobreajuste) fue el mínimo y las métricas las más altas. Los pesos no se modificaron con respecto al notebook inicial."
      ],
      "metadata": {
        "id": "-ncZB886wl5y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_lr = 3e-2\n",
        "epoch = 15\n",
        "weight_decay = 1e-4\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=max_lr, weight_decay=weight_decay)\n",
        "sched = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr, epochs=epoch,\n",
        "                                            steps_per_epoch=len(train_dataloader))\n",
        "\n",
        "history = fit(epoch, model, train_dataloader, val_dataloader, criterion, optimizer, sched)"
      ],
      "metadata": {
        "id": "-LANV0FVwkFL",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-09-17T16:15:17.579973Z",
          "iopub.execute_input": "2025-09-17T16:15:17.580189Z",
          "iopub.status.idle": "2025-09-17T16:19:19.034509Z",
          "shell.execute_reply.started": "2025-09-17T16:15:17.580172Z",
          "shell.execute_reply": "2025-09-17T16:19:19.033708Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model, 'Unet-efficientnet.pt')"
      ],
      "metadata": {
        "id": "Pf9thB19wo8M",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-09-17T16:19:19.035377Z",
          "iopub.execute_input": "2025-09-17T16:19:19.035605Z",
          "iopub.status.idle": "2025-09-17T16:19:19.168681Z",
          "shell.execute_reply.started": "2025-09-17T16:19:19.035589Z",
          "shell.execute_reply": "2025-09-17T16:19:19.168141Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Resultados del entrenamiento**\n",
        "\n",
        "Después del entrenamiento del modelo, es importante ver las gráficas con respecto a la perdida, la media de ioU y la accuracy, que nos ayudan a saber como fue el comportamiento del entrenamiento y el ajuste que fue teniendo durante cada epoch."
      ],
      "metadata": {
        "id": "PEQ6VlQxwuDq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_loss(history):\n",
        "    plt.plot(history['val_loss'], label='val', marker='o')\n",
        "    plt.plot( history['train_loss'], label='train', marker='o')\n",
        "    plt.title('Loss per epoch'); plt.ylabel('loss');\n",
        "    plt.xlabel('epoch')\n",
        "    plt.legend(), plt.grid()\n",
        "    plt.show()\n",
        "\n",
        "def plot_score(history):\n",
        "    plt.plot(history['train_miou'], label='train_mIoU', marker='*')\n",
        "    plt.plot(history['val_miou'], label='val_mIoU',  marker='*')\n",
        "    plt.title('Score per epoch'); plt.ylabel('mean IoU')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.legend(), plt.grid()\n",
        "    plt.show()\n",
        "\n",
        "def plot_acc(history):\n",
        "    plt.plot(history['train_acc'], label='train_accuracy', marker='*')\n",
        "    plt.plot(history['val_acc'], label='val_accuracy',  marker='*')\n",
        "    plt.title('Accuracy per epoch'); plt.ylabel('Accuracy')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.legend(), plt.grid()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "oCDCgY0dw6Kr",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-09-17T16:19:19.169777Z",
          "iopub.execute_input": "2025-09-17T16:19:19.170381Z",
          "iopub.status.idle": "2025-09-17T16:19:19.175875Z",
          "shell.execute_reply.started": "2025-09-17T16:19:19.170353Z",
          "shell.execute_reply": "2025-09-17T16:19:19.175325Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "plot_loss(history)\n",
        "plot_score(history)\n",
        "plot_acc(history)"
      ],
      "metadata": {
        "id": "wvdvteIzw6_H",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-09-17T16:19:19.176639Z",
          "iopub.execute_input": "2025-09-17T16:19:19.176959Z",
          "iopub.status.idle": "2025-09-17T16:19:19.720765Z",
          "shell.execute_reply.started": "2025-09-17T16:19:19.176936Z",
          "shell.execute_reply": "2025-09-17T16:19:19.720071Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Evaluación de los resultados del entrenamiento**\n",
        "\n",
        "**Loss:** The first graph shows the behavior of the loss over epochs for both training and validation. The loss seems extremely high in the first epoch of validation and then drops almost to zero. This indicates that the model quickly adjusts to the images.\n",
        "\n",
        "**mIoU:**\n",
        "\n",
        "**Accuracy:**"
      ],
      "metadata": {
        "id": "ewDiYe3QvEMn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**mIoU (Mean Intersection over Union)**: The second graph shows the mIoU, which measures the overlap between the predicted and ground truth areas for segmentation. This metric is particularly important in image segmentation tasks because it is more robust for evaluating the overlap between the segmented and actual areas. Both training and validation sets have some variation before the 8th epoch, where both start to increase steadily."
      ],
      "metadata": {
        "id": "2h3xadFBvS8h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Accuracy**: The third graph displays the model’s accuracy, measuring the percentage of correctly segmented pixels. The results show a small gap between training accuracy. Although accuracy is useful, it is not ideal for segmentation tasks where the background class (non-lesion) might dominate the evaluation."
      ],
      "metadata": {
        "id": "z2pK_8uvvPH1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Resultados**\n",
        "\n",
        "Creamos la funcion **predict_image_mask_miou** para cambiar el modelo a modo de evaluación y detener el entrenamiento, para probar las predicciones del modelo, de igual forma calcula los scores de mIoU y regresa la máscara de prediccion y el score final."
      ],
      "metadata": {
        "id": "UAJdttrPd0bX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_image_mask_miou(model, image, mask, mean=[0.485], std=[0.229]):\n",
        "    model.eval()\n",
        "    model.to(device); image=image.to(device)\n",
        "    mask = mask.to(device)\n",
        "\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output = model(image)\n",
        "        a,b,c,d = output.shape\n",
        "        score = mIoU(output, mask)\n",
        "        masked = torch.argmax(output, dim=1)\n",
        "        masked = masked.cpu().squeeze(0)\n",
        "    return masked, score, output.permute(0, 2, 3,1)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "hVAcEQrwY96M",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-09-17T16:19:19.721697Z",
          "iopub.execute_input": "2025-09-17T16:19:19.721994Z",
          "iopub.status.idle": "2025-09-17T16:19:19.72653Z",
          "shell.execute_reply.started": "2025-09-17T16:19:19.721976Z",
          "shell.execute_reply": "2025-09-17T16:19:19.725949Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "La funcion **predict_image_mask_pixel** sirve para algo similar a la anterior, pero en vez de calcular los scores de mIoU, revisa cuantos pixeles de manera individual fueron predichos de manera correcta usando **pixel_accuracy**"
      ],
      "metadata": {
        "id": "Fb8RUiYIftvt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_image_mask_pixel(model, image, mask, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]):\n",
        "    model.eval()\n",
        "    model.to(device); image=image.to(device)\n",
        "    mask = mask.to(device)\n",
        "    with torch.no_grad():\n",
        "\n",
        "        output = model(image)\n",
        "        acc = pixel_accuracy(output, mask)\n",
        "        masked = torch.argmax(output, dim=1)\n",
        "        masked = masked.cpu().squeeze(0)\n",
        "    return masked, acc\n"
      ],
      "metadata": {
        "id": "PCXk8Qq2fo2a",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-09-17T16:19:19.727292Z",
          "iopub.execute_input": "2025-09-17T16:19:19.727565Z",
          "iopub.status.idle": "2025-09-17T16:19:19.742443Z",
          "shell.execute_reply.started": "2025-09-17T16:19:19.727542Z",
          "shell.execute_reply": "2025-09-17T16:19:19.741852Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "La función **mask_to_onehot** convierte la máscara de colores en un one-hot encoding."
      ],
      "metadata": {
        "id": "Gyj2yQQrgIgg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def mask_to_onehot(mask, palette):\n",
        "    \"\"\"\n",
        "    Converts a segmentation mask (H, W, C) to (H, W, K) where the last dim is a one\n",
        "    hot encoding vector, C is usually 1 or 3, and K is the number of class.\n",
        "    \"\"\"\n",
        "    semantic_map = []\n",
        "    for colour in palette:\n",
        "        equality = np.equal(mask, colour)\n",
        "        class_map = np.all(equality, axis=-1)\n",
        "        semantic_map.append(class_map)\n",
        "    semantic_map = np.stack(semantic_map, axis=-1).astype(np.float32)\n",
        "    return torch.from_numpy(semantic_map)"
      ],
      "metadata": {
        "id": "8vg-_Zu3frJN",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-09-17T16:19:19.743244Z",
          "iopub.execute_input": "2025-09-17T16:19:19.743432Z",
          "iopub.status.idle": "2025-09-17T16:19:19.756731Z",
          "shell.execute_reply.started": "2025-09-17T16:19:19.743417Z",
          "shell.execute_reply": "2025-09-17T16:19:19.756037Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Usando las funciones anteriores, obtenemos un batch de imágenes con sus máscaras del dstaset de validación y pasamos las imagenes a travez de modelo para predecir su máscara. También sacamos su score mIoU para saber que tan bien las máscaras predichas coinciden con las máscaras reales y convertir la máscara de referencia a one-hot encoding."
      ],
      "metadata": {
        "id": "xldmAvaQhZ9n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image, mask = next(iter(val_dataloader))"
      ],
      "metadata": {
        "id": "sCT30wpNi_Dl",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-09-17T16:19:19.757561Z",
          "iopub.execute_input": "2025-09-17T16:19:19.758064Z",
          "iopub.status.idle": "2025-09-17T16:19:19.794877Z",
          "shell.execute_reply.started": "2025-09-17T16:19:19.75804Z",
          "shell.execute_reply": "2025-09-17T16:19:19.794054Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "pred_mask, score,output = predict_image_mask_miou(model, image, mask)\n",
        "semantic_map = mask_to_onehot(torch.unsqueeze(mask,-1).cpu().numpy(),palette)"
      ],
      "metadata": {
        "id": "sozS-u6ZZBYx",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-09-17T16:19:19.798363Z",
          "iopub.execute_input": "2025-09-17T16:19:19.798578Z",
          "iopub.status.idle": "2025-09-17T16:19:19.91851Z",
          "shell.execute_reply.started": "2025-09-17T16:19:19.798562Z",
          "shell.execute_reply": "2025-09-17T16:19:19.917861Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **RESULTADOS VISUALES**"
      ],
      "metadata": {
        "id": "5Ryp3r9KjnIQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "visualize(image, semantic_map, pred_batch=output)"
      ],
      "metadata": {
        "id": "EbNNM-mmZDyS",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-09-17T16:19:19.919307Z",
          "iopub.execute_input": "2025-09-17T16:19:19.919583Z",
          "iopub.status.idle": "2025-09-17T16:19:21.424546Z",
          "shell.execute_reply.started": "2025-09-17T16:19:19.919555Z",
          "shell.execute_reply": "2025-09-17T16:19:21.423647Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "**INTERPRETACIÓN DE LOS GRÁFICOS**\n",
        "\n",
        "-\tAmarillo: Pixeles donde tanto la máscara de predicción como la máscara verdadera están activadas para una clase específica. El modelo predijo correctamente para esos pixeles.\n",
        "-\tRojo: Pixeles donde el modelo predice una clase pero la máscara verdadera es negativa. El modelo predijo incorrectamente que estos pixeles pertenecen a una clase cuando en realidad no.\n",
        "-\tVerde: Pixeles donde la máscara de verdad si dice que pertenecen a una clase pero la predicción no identifico que esos pixeles correspondían a una clase.\n",
        "\n",
        "Podemos observar que una gran parte de los pixeles esta en amarillo lo que significa que los valores que fueron correctamente predichos con respecto a la máscara de referencia, por otro lado tambien observamos que existen muchos pixeles en rojo lo que significa que hay muchos falsos positivos en las predicciones del modelo."
      ],
      "metadata": {
        "id": "QlUtH0bzoiyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **USO DEL MODELO EN DATASET DE PRUEBA**\n",
        "\n",
        "Después de analizar el desempeño del modelo creamos la función **miou_score** para cad imagen del dataset de validation y test. Es un loop que pasa por el dataset y predice las máscaras de clases y calcula su score guardandolo en una lista que nos va a permitir ver el desempeño del modelo en el dataset del test."
      ],
      "metadata": {
        "id": "adVGCQe0jtat"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def miou_score(model, test_set):\n",
        "    score_iou = []\n",
        "    for i, data in enumerate(tqdm(test_set)):\n",
        "        img, mask = data\n",
        "        pred_mask, score,output = predict_image_mask_miou(model, img, mask)\n",
        "        score_iou.append(score)\n",
        "    return score_iou\n",
        "\n",
        "mob_miou = miou_score(model, val_dataloader)\n",
        "mob_miou\n",
        "\n"
      ],
      "metadata": {
        "id": "izVchv-MdMTe",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-09-17T16:19:21.42537Z",
          "iopub.execute_input": "2025-09-17T16:19:21.425685Z",
          "iopub.status.idle": "2025-09-17T16:19:21.577557Z",
          "shell.execute_reply.started": "2025-09-17T16:19:21.425664Z",
          "shell.execute_reply": "2025-09-17T16:19:21.5768Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Limpiamos los datos de entrenamiento"
      ],
      "metadata": {
        "id": "va6xbFzzl7lh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "del train_images\n",
        "del train_masks"
      ],
      "metadata": {
        "id": "JbX06MRAmAHO",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-09-17T16:19:21.578376Z",
          "iopub.execute_input": "2025-09-17T16:19:21.578678Z",
          "iopub.status.idle": "2025-09-17T16:19:21.582398Z",
          "shell.execute_reply.started": "2025-09-17T16:19:21.57866Z",
          "shell.execute_reply": "2025-09-17T16:19:21.581438Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TEST PREDICT**\n",
        "\n",
        "Creamos la funcion **test_predict** para normalizar y predecir la máscara de cada una de las imágenes del dataset de test. Usamos la función de **softmax** para normalizar el output de probabilidades y redimencionamos la máscara de predicción para que tenga las mismas dimensiones esperadas."
      ],
      "metadata": {
        "id": "rLUtdevkl5kw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test_predict(model, image, mean=[0.485], std=[0.229]):\n",
        "    t = T.Compose([T.ToTensor(), T.Normalize(mean, std)])\n",
        "    image = t(image)\n",
        "    model.eval()\n",
        "\n",
        "    model.to(device); image=image.to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output = model(torch.unsqueeze(image,1))\n",
        "        output = nn.Softmax(dim=1)(output)\n",
        "    return output.permute(0, 2, 3,1)"
      ],
      "metadata": {
        "id": "G2E9KfEPdP-h",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-09-17T16:19:21.583222Z",
          "iopub.execute_input": "2025-09-17T16:19:21.583476Z",
          "iopub.status.idle": "2025-09-17T16:19:21.59693Z",
          "shell.execute_reply.started": "2025-09-17T16:19:21.583453Z",
          "shell.execute_reply": "2025-09-17T16:19:21.596205Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "**VISUALIZACION DE RESULTADOS DE PRUEBA**\n",
        "\n",
        "Procesamos un batch de imagenes del dataset de test y sus máscaras de predicción, las convertimos de tensors a arrays. Las máscaras predichas tienen un umbral de 0,5 y se muestran junto con las imágenes de entrada."
      ],
      "metadata": {
        "id": "ivp9tp3HmZvM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image_batch = np.stack([val_augs(image=img)['image'] for img in test_images_medseg], axis=0)\n",
        "print(torch.from_numpy(image_batch).shape)\n",
        "print(image_batch[i].shape)\n",
        "#output = test_predict(model, torch.from_numpy(image_batch).permute(0, 3, 1,2))\n",
        "output = np.zeros((10,256,256,4))\n",
        "for i in range(10):\n",
        "    output[i] = test_predict(model, image_batch[i]).cpu().numpy()\n",
        "print(output.shape)\n",
        "test_masks_prediction = output > 0.5\n",
        "visualize(image_batch, test_masks_prediction, num_samples=len(test_images_medseg))"
      ],
      "metadata": {
        "id": "Z4z-VSE5dSuJ",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-09-17T16:19:21.597824Z",
          "iopub.execute_input": "2025-09-17T16:19:21.598076Z",
          "iopub.status.idle": "2025-09-17T16:19:24.048318Z",
          "shell.execute_reply.started": "2025-09-17T16:19:21.598054Z",
          "shell.execute_reply": "2025-09-17T16:19:24.047622Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finalmente redimencionamos las máscaras predichas usando scipy, para agrandarlas para que coincidan con el tamaño o la resolución de la imagen original."
      ],
      "metadata": {
        "id": "a1bcuiL_7flv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import scipy\n",
        "test_masks_prediction_original_size = scipy.ndimage.zoom(test_masks_prediction[..., :-2], (1, 2, 2, 1), order=0)\n",
        "test_masks_prediction_original_size.shape"
      ],
      "metadata": {
        "id": "UUoN92Vl7WFO",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-09-17T16:19:24.049139Z",
          "iopub.execute_input": "2025-09-17T16:19:24.049392Z",
          "iopub.status.idle": "2025-09-17T16:19:24.140778Z",
          "shell.execute_reply.started": "2025-09-17T16:19:24.049372Z",
          "shell.execute_reply": "2025-09-17T16:19:24.140268Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aplanamos las máscaras predichas a una matriz unidimensional, transformamos los datos en un DataFrame y lo guardamos como un archivo CSV, el contenido del archivo tendrá el índice y los valores de máscara predichos para cada píxel."
      ],
      "metadata": {
        "id": "8siWmZ-k7w4e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "pd.DataFrame(\n",
        "             data=np.stack((np.arange(len(test_masks_prediction_original_size.ravel())),\n",
        "                            test_masks_prediction_original_size.ravel().astype(int)),\n",
        "                            axis=-1),\n",
        "             columns=['Id', 'Predicted'])\\\n",
        ".set_index('Id').to_csv('sub.csv')"
      ],
      "metadata": {
        "id": "H5WJL-ub7SOz",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-09-17T16:19:24.141479Z",
          "iopub.execute_input": "2025-09-17T16:19:24.142044Z",
          "iopub.status.idle": "2025-09-17T16:19:27.408201Z",
          "shell.execute_reply.started": "2025-09-17T16:19:24.142015Z",
          "shell.execute_reply": "2025-09-17T16:19:27.407583Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusión**\n",
        "\n",
        "El modelo desarrollado demostró una capacidad notable para identificar regiones infectadas en imágenes CT de pulmones, segmentando áreas extensas y también pequeñas para un diagnóstico médico inicial. Los resultados visuales y cuantitativos confirman que la arquitectura U-Net++ con EfficientNet-B2 podria ser adecuada para esta tarea, capturando características clave como opacidades en vidrio esmerilado y consolidaciones.\n",
        "\n",
        "Sin embargo, se observaron algunas limitaciones:\n",
        "\n",
        "- **Falsos positivos (áreas rojas)**: El modelo en ocasiones marca como infectadas regiones sanas, posiblemente por un sobreajuste (overfitting) a patrones específicos del dataset de train o a similitudes visuales entre tejido sano e infectado.\n",
        "- **Falsos negativos (áreas verdes)**: Aunque menos frecuentes, el modelo omite algunas áreas infectadas, especialmente cuando son pequeñas o presentan baja visibilidad.\n",
        "\n",
        "Estos errores significan que es necesario mejorar la generalización del modelo y refinar su sensibilidad a características sutiles.\n",
        "\n",
        "**Posibles mejoras:**\n",
        "\n",
        "- Ajustar la loss function para enfocarse mejor en clases minoritarias (lesiones pequeñas).\n",
        "- Incorporar técnicas de post-procesamiento para reducir falsos positivos.\n",
        "- Buscar métodos de aumento de datos más avanzados para más diversidad y balance.\n",
        "\n",
        "En general, el modelo tiene potencial para ser una buena herramienta para la segmentación de imágenes CT de COVID-19, pero requiere optimización para minimizar errores y garantizar su confiabilidad en entornos clínicos reales."
      ],
      "metadata": {
        "id": "XEqP0jupq4aF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **REFERENCIAS**\n",
        "\n",
        "- Società Italiana di Radiologia Medica e Interventistica. (s.f.). Home. SIRM. Recuperado 14 de septiembre de 2024, de\n",
        "https://sirm.org/en/\n",
        "\n",
        "- Gaillard, F., & Weerakkody, Y. (s.f.). *COVID-19*. Radiopaedia.org. Recuperado 14 de septiembre de 2024, de\n",
        "https://radiopaedia.org/articles/covid-19-4?lang=us\n",
        "\n",
        "- Ronneberger, O., Fischer, P., & Brox, T. (2015). U-Net: Convolutional Networks for Biomedical Image Segmentation. arXiv.\n",
        "https://doi.org/10.48550/arXiv.1506.01186\n",
        "\n",
        "- GeekforGeeks. (s.f.). U-Net Architecture Explained. Recuperado 14 de septiembre de 2024, de\n",
        "https://www.geeksforgeeks.org/machine-learning/unet-architecture-explained/"
      ],
      "metadata": {
        "id": "h-2lJ5uIAK76"
      }
    }
  ]
}